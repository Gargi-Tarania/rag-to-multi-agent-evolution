{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9fb2fcbf",
      "metadata": {
        "id": "9fb2fcbf"
      },
      "source": [
        "# LangChain End-to-End: Prompt → Chain → RAG → Agent → UI\n",
        "\n",
        "This single notebook merges the **basics** and **agent** demos into one coherent flow:\n",
        "\n",
        "1. **Hello LLM** (baseline)  \n",
        "2. **Prompting + LCEL + Output Parser**  \n",
        "3. **RAG (build once, re-use)** with sources  \n",
        "4. **Agent + Tools** (retriever tool + optional web search)  \n",
        "5. **Tiny UI** (Gradio)  \n",
        "6. **Custom Tool (@tool) example**  \n",
        "\n",
        "> **Tip:** Open the notebook command palette and run all cells, or step through section by section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dd1a9bd",
      "metadata": {
        "id": "4dd1a9bd"
      },
      "source": [
        "## 0) Install & Configure (run once)\n",
        "\n",
        "- Installs pinned versions to reduce API drift.  \n",
        "- Prompts for your keys as needed.  \n",
        "- **Optional**: If you have a LangSmith key, tracing will be enabled automatically.\n",
        "\n",
        "> If you re-run the notebook later, you can skip re-installation if your environment already has these packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90f34aca",
      "metadata": {
        "id": "90f34aca"
      },
      "outputs": [],
      "source": [
        "# Install LangChain & LangGraph v1 with compatible integrations\n",
        "# Keep requests >= 2.32.5 to satisfy langchain-community (Colab may warn; it's OK)\n",
        "%pip install -qU \\\n",
        "    \"requests>=2.32.5\" \\\n",
        "    \"langchain>=1.0.3,<1.1\" \\\n",
        "    \"langgraph>=1.0,<2\" \\\n",
        "    \"langchain-openai>=1.0\" \\\n",
        "    \"langchain-community>=0.4,<1.0\" \\\n",
        "    \"langchain-text-splitters>=1.0.0\" \\\n",
        "    beautifulsoup4 lxml faiss-cpu langchainhub tavily-python \"gradio>=4.0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ekm9gCUx2m-B"
      },
      "outputs": [],
      "source": [
        "import importlib\n",
        "def _ver(name):\n",
        "    try:\n",
        "        m = importlib.import_module(name)\n",
        "        return getattr(m, \"__version__\", \"n/a\")\n",
        "    except Exception as e:\n",
        "        return f\"not installed ({e})\"\n",
        "print(\"langchain           :\", _ver(\"langchain\"))\n",
        "print(\"langgraph           :\", _ver(\"langgraph\"))\n",
        "print(\"langchain-core      :\", _ver(\"langchain_core\"))\n",
        "print(\"langchain-community :\", _ver(\"langchain_community\"))\n",
        "print(\"langchain-openai    :\", _ver(\"langchain_openai\"))\n",
        "print(\"langchainhub        :\", _ver(\"langchainhub\"))\n",
        "print(\"langchain-text-splitters:\", _ver(\"langchain_text_splitters\"))\n",
        "print(\"faiss-cpu           :\", _ver(\"faiss\"))\n",
        "print(\"tavily-python       :\", _ver(\"tavily\"))\n"
      ],
      "id": "Ekm9gCUx2m-B"
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "YbeRxYtfvMFW"
      },
      "id": "YbeRxYtfvMFW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e200643c"
      },
      "source": [
        "### Loading Environment Variables from a `.env` File\n",
        "\n",
        "While this notebook uses `os.getenv` and `getpass` to manage API keys, if you prefer using a `.env` file for local development or consistency, you can install and use the `python-dotenv` library."
      ],
      "id": "e200643c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9c08795"
      },
      "source": [
        "# Install python-dotenv\n",
        "%pip install python-dotenv"
      ],
      "id": "c9c08795",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eae87bf7"
      },
      "source": [
        "Next, create a `.env` file in the root of your Colab environment. You can use the `%%writefile` magic command for this. Replace `YOUR_VALUE_HERE` with your actual key or value."
      ],
      "id": "eae87bf7"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os"
      ],
      "metadata": {
        "id": "LBIAaBHItO87"
      },
      "id": "LBIAaBHItO87",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"]=userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"]=userdata.get('LANGSMITH_API_KEY')\n",
        "os.environ[\"TAVILY_API_KEY\"]=userdata.get('TAVILY_API_KEY')"
      ],
      "metadata": {
        "id": "3ooeIkQmtdsS"
      },
      "id": "3ooeIkQmtdsS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile .env\n",
        "# OPENAI_API_KEY=OPENAI_API_KEY\n",
        "# LANGCHAIN_TRACING_V2=true\n",
        "# LANGCHAIN_API_KEY=LANGCHAIN_API_KEY\n",
        "# TAVILY_API_KEY=TAVILY_API_KEY"
      ],
      "metadata": {
        "id": "OnKee_97Q2KQ"
      },
      "id": "OnKee_97Q2KQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e1546f5"
      },
      "source": [
        "# import os\n",
        "\n",
        "# # Assuming .env is in the current working directory (root of Colab session)\n",
        "# env_file_path = os.path.abspath('.env')\n",
        "# print(f\"The .env file is located at: {env_file_path}\")"
      ],
      "id": "2e1546f5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ce321a1"
      },
      "source": [
        "Now, load the environment variables from the `.env` file and access them using `os.getenv`:"
      ],
      "id": "3ce321a1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9147c0e9"
      },
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load variables from .env file\n",
        "# load_dotenv()\n",
        "\n",
        "# Access the variables\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "LANGCHAIN_TRACING_V2 = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
        "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
        "\n",
        "# print(f\"OPENAI_API_KEY: {OPENAI_API_KEY}\")\n",
        "# print(f\"LANGCHAIN_TRACING_V2: {LANGCHAIN_TRACING_V2}\")\n",
        "# print(f\"LANGCHAIN_API_KEY: {LANGCHAIN_API_KEY}\")\n",
        "# print(f\"TAVILY_API_KEY: {TAVILY_API_KEY}\")\n",
        "\n",
        "# You can then use these variables where needed, e.g., os.environ[\"OPENAI_API_KEY\"] = my_api_key"
      ],
      "id": "9147c0e9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NL6CTHFW2m-B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import List, Any\n",
        "from langchain.agents import create_agent\n",
        "from langchain_core.tools import create_retriever_tool\n",
        "try:\n",
        "    from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "except Exception:\n",
        "    TavilySearchResults = None\n",
        "\n",
        "DEFAULT_MODEL = os.getenv(\"LC_V1_MODEL\", \"gpt-4o-mini\")\n",
        "\n",
        "def build_v1_agent(tools: List[Any], system_prompt: str = \"You are a helpful assistant.\"):\n",
        "    # In v1, `model` can be a string model id (e.g., 'gpt-4o-mini') or a chat model instance.\n",
        "    return create_agent(model=DEFAULT_MODEL, tools=tools, system_prompt=system_prompt)\n",
        "\n",
        "def run_agent(agent, question: str):\n",
        "    try:\n",
        "        return agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": question}]})\n",
        "    except Exception:\n",
        "        return agent.invoke(question)\n"
      ],
      "id": "NL6CTHFW2m-B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1658930",
      "metadata": {
        "id": "a1658930"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, getpass, warnings, sys\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def ensure_env(key: str, prompt: str):\n",
        "    if not os.getenv(key):\n",
        "        try:\n",
        "            val = getpass.getpass(prompt)\n",
        "        except Exception:\n",
        "            # Fallback for environments without stdin (e.g. some hosted notebooks)\n",
        "            val = \"\"\n",
        "        if val:\n",
        "            os.environ[key] = val\n",
        "\n",
        "# --- Required for LLM & embeddings ---\n",
        "ensure_env(\"OPENAI_API_KEY\", \"Enter your OpenAI API Key (skipped if already set): \")\n",
        "\n",
        "# --- Optional: LangSmith tracing ---\n",
        "# If you have LANGCHAIN_API_KEY set, we turn on tracing v2 automatically.\n",
        "if os.getenv(\"LANGCHAIN_API_KEY\"):\n",
        "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "    print(\"LangSmith tracing enabled (TRACING_V2=true).\")\n",
        "else:\n",
        "    os.environ.pop(\"LANGCHAIN_TRACING_V2\", None)\n",
        "    print(\"LangSmith tracing not enabled (no LANGCHAIN_API_KEY). Proceeding without tracing.\")\n",
        "\n",
        "# --- Optional: Tavily web search ---\n",
        "# If not set, we'll skip adding the Tavily tool; everything else runs fine.\n",
        "if not os.getenv(\"TAVILY_API_KEY\"):\n",
        "    print(\"No TAVILY_API_KEY found. Agent will run without web search tool (retriever-only).\")\n",
        "else:\n",
        "    print(\"Tavily web search tool will be available.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78fe3e93",
      "metadata": {
        "id": "78fe3e93"
      },
      "source": [
        "## 1) Hello LLM (baseline)\n",
        "\n",
        "A single LLM call; no structure, no grounding.  \n",
        "We'll improve over this baseline throughout the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c00388f8",
      "metadata": {
        "id": "c00388f8"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# A deterministic model (temperature=0) for reproducible outputs.\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "baseline_q = \"In one sentence, how can LangSmith help with testing LLM apps?\"\n",
        "baseline_a = llm.invoke(baseline_q)\n",
        "print(\"Q:\", baseline_q)\n",
        "print(\"\\nBaseline (no context):\\n\", baseline_a.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "180fe669",
      "metadata": {
        "id": "180fe669"
      },
      "source": [
        "## 2) Prompting + LCEL (LangChain Expression Language) + Output Parsing\n",
        "\n",
        "Use **LCEL** (`|`) to pipe **PromptTemplate → LLM → OutputParser** so your code is composable and testable."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys_prompt = \"\"\"\n",
        "\n",
        "# Guideline\n",
        "--------------\n",
        "1.\n",
        "2.\n",
        "3.\n",
        "\n",
        "# Task\n",
        "classification of movie review\n",
        "\n",
        "# Global Flows\n",
        "\n",
        "# Do's\n",
        "\n",
        "# Don't\n",
        "\n",
        "# Output Requirements\n",
        "-------\n",
        "1. one word: positive, negative\n",
        "2. The sentiment of the kv9e is:\n",
        "3. {\n",
        "  \"movie\":\n",
        "  \"sentiment:\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "query: \"avatar was the most amaxing \""
      ],
      "metadata": {
        "id": "RcF1moyxKd8f"
      },
      "id": "RcF1moyxKd8f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04140620",
      "metadata": {
        "id": "04140620"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate   # ✅ v1 path\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# LLM (uses OPENAI_API_KEY from env)\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# Prompt → LLM → Parser, piped with LCEL `|`\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a concise technical assistant.\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Try it\n",
        "chain.invoke({\"question\": \"In one sentence, what does LCEL do?\"})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c55e856a",
      "metadata": {
        "id": "c55e856a"
      },
      "source": [
        "## 3) RAG\n",
        "\n",
        "We’ll load a small corpus (LangSmith docs), split it, embed it, index it with **FAISS**, and wire a **Retrieval Chain**.\n",
        "\n",
        "- This section runs **once** and is reused later by the Agent.\n",
        "- If web loading fails, we fall back to a tiny local sample so the demo still runs.\n",
        "- We'll also **surface sources** so you can see why answers improved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9be9594",
      "metadata": {
        "id": "d9be9594"
      },
      "outputs": [],
      "source": [
        "# RAG (v1): Web loader → splitter → FAISS → retriever → LCEL chain\n",
        "import os\n",
        "os.environ.setdefault(\"USER_AGENT\", \"IK-LangChain-RAG/1.0 (contact: ops@your-org)\")  # fixes the warning\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 1) Load docs (pick any public pages you want indexed)\n",
        "urls = [\n",
        "    \"https://python.langchain.com/docs/get_started/introduction/\",\n",
        "    \"https://docs.smith.langchain.com/\"\n",
        "]\n",
        "loader = WebBaseLoader(urls) # web scraping\n",
        "docs = loader.load()\n",
        "\n",
        "# 2) Chunk\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "# 3) Embed & index\n",
        "emb = OpenAIEmbeddings()  # uses OPENAI_API_KEY from env\n",
        "vs = FAISS.from_documents(chunks, emb)\n",
        "retriever = vs.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# 4) Prompt (stuff-style: we inject all retrieved chunks into {context})\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"You are a precise assistant. Use the provided CONTEXT to answer.\\n\"\n",
        "     \"If the answer isn't in the context, say you don't know.\\n\\nCONTEXT:\\n{context}\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "# 5) LCEL pipeline: {question} flows through; {context} is produced by retriever\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 6) Try it\n",
        "rag_chain.invoke(\"What is LangSmith and how does it relate to LangChain?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke(\"What is LangSmith and how does it relate to LangChain?\")\n",
        "# .(\"What is LangSmith and how does it relate to LangChain\")"
      ],
      "metadata": {
        "id": "aBlse6_9QQmX"
      },
      "id": "aBlse6_9QQmX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It helps you trace requests, evaluate outputs, test prompts, and manage deployments in one place.\\nLangSmith is framework agnostic, so you can use it with or without LangChain’s open-source libraries\\nlangchain and langgraph.\\nPrototype locally, then move to production with integrated monitoring and evaluation to build more reliable AI systems.\\nLangGraph Platform is now LangSmith Deployment. For more information, check out the Changelog.\\n\\u200bGet started\\nCreate an accountSign up at smith.langchain.com (no credit card required).\\nYou can log in with Google, GitHub, or email.Create an API keyGo to your Settings page → API Keys → Create API Key.\\nCopy the key and save it securely.\\nOnce your account and\n",
        "# API key are ready, choose a quickstart to begin building with LangSmith:"
      ],
      "metadata": {
        "id": "KmoV7Yx-RbbG"
      },
      "id": "KmoV7Yx-RbbG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "accb47a8",
      "metadata": {
        "id": "accb47a8"
      },
      "source": [
        "## 4) Agent + Tools (on top of the same RAG)\n",
        "\n",
        "We turn our retriever into a **Tool** and create an **OpenAI Functions Agent**.  \n",
        "Optionally, if a **Tavily** API key is present, we add a web search tool.\n",
        "\n",
        "> **Why an Agent?** It can decide *when* to use retrieval vs. answer directly, and sequence multi-step reasoning."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tavily is a search engine specifically designed for AI agents and Large Language Models (LLMs).\n",
        "# It focuses on providing real-time, accurate, and factual information for AI-driven applications.\n",
        "# Unlike general-purpose search engines, Tavily prioritizes providing high-quality, concise, and readily usable data for AI to process.\n",
        "\n",
        "# List of other tools - https://docs.langchain.com/oss/python/integrations/tools"
      ],
      "metadata": {
        "id": "lch7CVlHTxCE"
      },
      "id": "lch7CVlHTxCE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93dad268",
      "metadata": {
        "id": "93dad268"
      },
      "outputs": [],
      "source": [
        "# Agent + Tools (LangChain v1) — Structured JSON output\n",
        "\n",
        "import os, json\n",
        "from typing import Any, Dict, List, Optional\n",
        "from pprint import pprint\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import create_agent\n",
        "from langchain_core.tools import tool, create_retriever_tool\n",
        "from langchain_core.messages import AIMessage, ToolMessage\n",
        "\n",
        "# Optional: Tavily web search tool\n",
        "try:\n",
        "    from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "except Exception:\n",
        "    TavilySearchResults = None\n",
        "\n",
        "# ------------------ Define tools ------------------\n",
        "tools: List[Any] = []\n",
        "\n",
        "@tool\n",
        "def add(a: float, b: float) -> float:\n",
        "    \"\"\"Add two numbers.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "tools.append(add)\n",
        "\n",
        "# add(5, 3)\n",
        "\n",
        "# Add retriever tool if your RAG cell created `retriever`\n",
        "if \"retriever\" in globals():\n",
        "    kb_tool = create_retriever_tool(\n",
        "        globals()[\"retriever\"],\n",
        "        name=\"kb_search\",\n",
        "        description=\"Search the indexed KB/curriculum docs and return relevant passages.\"\n",
        "    )\n",
        "    tools.append(kb_tool)\n",
        "\n",
        "# Optional Tavily tool (requires TAVILY_API_KEY)\n",
        "if TavilySearchResults and os.getenv(\"TAVILY_API_KEY\"):\n",
        "    tools.append(TavilySearchResults(max_results=5, include_answer=True))\n",
        "\n",
        "# ------------------ Build agent ------------------\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "agent = create_agent(\n",
        "    model=llm,\n",
        "    tools=tools,\n",
        "    system_prompt=(\n",
        "        \"You are a helpful technical assistant. Use tools when useful. \"\n",
        "        \"Prefer kb_search for questions about our course/KB; use web search for general web info.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "# ------------------ Helpers for structured output ------------------\n",
        "def _final_text(res: Any) -> str:\n",
        "    \"\"\"Return final assistant text from AIMessage or from a {messages:[...]} dict.\"\"\"\n",
        "    if isinstance(res, AIMessage):\n",
        "        return res.content or \"\"\n",
        "    if isinstance(res, dict) and \"messages\" in res:\n",
        "        for m in reversed(res[\"messages\"]):\n",
        "            if isinstance(m, AIMessage) or getattr(m, \"type\", \"\") == \"ai\":\n",
        "                return getattr(m, \"content\", \"\") or \"\"\n",
        "    return str(res)\n",
        "\n",
        "def _collect_tool_calls_and_outputs(res: Any, max_len: int = 500) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Return [{'name','args','output'}...] by pairing tool calls to ToolMessage outputs.\"\"\"\n",
        "    messages: List[Any] = []\n",
        "    if isinstance(res, dict) and \"messages\" in res:\n",
        "        messages = res[\"messages\"]\n",
        "\n",
        "    # Find first AI message with tool_calls (some runtimes store it in .tool_calls, others in additional_kwargs)\n",
        "    tool_calls: List[Dict[str, Any]] = []\n",
        "    for m in messages:\n",
        "        if isinstance(m, AIMessage) and getattr(m, \"tool_calls\", None):\n",
        "            tool_calls = m.tool_calls or []\n",
        "            break\n",
        "        addkw = getattr(m, \"additional_kwargs\", {}) if hasattr(m, \"additional_kwargs\") else {}\n",
        "        if addkw.get(\"tool_calls\"):\n",
        "            tool_calls = addkw[\"tool_calls\"]\n",
        "            break\n",
        "\n",
        "    # Map tool_call_id -> output from ToolMessage\n",
        "    outputs: Dict[str, str] = {}\n",
        "    for m in messages:\n",
        "        if isinstance(m, ToolMessage):\n",
        "            outputs[getattr(m, \"tool_call_id\", None)] = (getattr(m, \"content\", \"\") or \"\")\n",
        "\n",
        "    def trunc(s: Optional[str]) -> str:\n",
        "        if not s:\n",
        "            return \"\"\n",
        "        return s[:max_len] + (\"…\" if len(s) > max_len else \"\")\n",
        "\n",
        "    structured: List[Dict[str, Any]] = []\n",
        "    for c in tool_calls:\n",
        "        structured.append({\n",
        "            \"name\": c.get(\"name\"),\n",
        "            \"args\": c.get(\"args\"),\n",
        "            \"output\": trunc(outputs.get(c.get(\"id\")))\n",
        "        })\n",
        "    return structured\n",
        "\n",
        "def _usage(res: Any) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"Best-effort token usage extraction.\"\"\"\n",
        "    if isinstance(res, AIMessage):\n",
        "        return getattr(res, \"response_metadata\", {}).get(\"token_usage\")\n",
        "    if isinstance(res, dict) and \"messages\" in res:\n",
        "        for m in reversed(res[\"messages\"]):\n",
        "            meta = getattr(m, \"response_metadata\", {}) if hasattr(m, \"response_metadata\") else {}\n",
        "            if meta.get(\"token_usage\"):\n",
        "                return meta[\"token_usage\"]\n",
        "    return None\n",
        "\n",
        "def to_structured(res: Any) -> Dict[str, Any]:\n",
        "    return {\n",
        "        \"answer\": _final_text(res),\n",
        "        \"tools\": _collect_tool_calls_and_outputs(res),\n",
        "        \"usage\": _usage(res),\n",
        "    }\n",
        "\n",
        "# ------------------ Run and show structured JSON ------------------\n",
        "query = \"What is 41 + 1? Also, if I ask about LangGraph later, how would you use kb_search?\"\n",
        "res = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]})\n",
        "pprint(to_structured(res), width=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]})"
      ],
      "metadata": {
        "id": "D7cde45xwCtu"
      },
      "id": "D7cde45xwCtu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91bbcbe0",
      "metadata": {
        "id": "91bbcbe0"
      },
      "outputs": [],
      "source": [
        "# Run multiple queries through the v1 agent and print tidy results\n",
        "\n",
        "from pprint import pprint\n",
        "from langchain_core.messages import AIMessage\n",
        "\n",
        "queries = [\n",
        "    \"List two LangSmith capabilities that support evaluation and how to use them.\",\n",
        "    \"Where do the docs explain tracing? Summarize in 3 bullets.\",\n",
        "]\n",
        "\n",
        "def _final_text(res):\n",
        "    if isinstance(res, AIMessage):\n",
        "        return res.content or \"\"\n",
        "    if isinstance(res, dict) and \"messages\" in res:\n",
        "        for m in reversed(res[\"messages\"]):\n",
        "            if isinstance(m, AIMessage) or getattr(m, \"type\", \"\") == \"ai\":\n",
        "                return getattr(m, \"content\", \"\") or \"\"\n",
        "    return str(res)\n",
        "\n",
        "for q in queries:\n",
        "    print(\"\\n\" + \"=\" * 26)\n",
        "    print(\"AGENT Q:\", q)\n",
        "\n",
        "    # v1 agents expect a messages list; fall back to raw string if needed\n",
        "    try:\n",
        "        res = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": q}]})\n",
        "    except Exception:\n",
        "        res = agent.invoke(q)\n",
        "\n",
        "    # If you used Option 2 earlier, show structured JSON; else print final text\n",
        "    if \"to_structured\" in globals():\n",
        "        pprint(to_structured(res), width=100)\n",
        "    else:\n",
        "        print(\"\\nAGENT A:\\n\", _final_text(res))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what are some events happening in new york this weekend\"}]})\n",
        "pprint(to_structured(res), width=100)"
      ],
      "metadata": {
        "id": "6x3hegNkZ4Cy"
      },
      "id": "6x3hegNkZ4Cy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e5ddd5a1",
      "metadata": {
        "id": "e5ddd5a1"
      },
      "source": [
        "## 5) Tiny UI (Gradio)\n",
        "\n",
        "A minimal chat interface that routes user messages to the agent.  \n",
        "If Tavily is not available, the agent still works with the retriever tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "591c43ed",
      "metadata": {
        "id": "591c43ed"
      },
      "outputs": [],
      "source": [
        "\n",
        "import gradio as gr\n",
        "from langchain_core.messages import AIMessage\n",
        "\n",
        "# -- helpers --\n",
        "def _final_text(res):\n",
        "    if isinstance(res, AIMessage):\n",
        "        return res.content or \"\"\n",
        "    if isinstance(res, dict) and \"messages\" in res:\n",
        "        for m in reversed(res[\"messages\"]):\n",
        "            if isinstance(m, AIMessage) or getattr(m, \"type\", \"\") == \"ai\":\n",
        "                return getattr(m, \"content\", \"\") or \"\"\n",
        "    return str(res)\n",
        "\n",
        "def _to_messages(history, message):\n",
        "    # gr.ChatInterface history is List[Tuple[user, assistant]]\n",
        "    msgs = []\n",
        "    for u, a in history:\n",
        "        if u: msgs.append({\"role\": \"user\", \"content\": u})\n",
        "        if a: msgs.append({\"role\": \"assistant\", \"content\": a})\n",
        "    msgs.append({\"role\": \"user\", \"content\": message})\n",
        "    return msgs\n",
        "\n",
        "def _ensure_agent():\n",
        "    \"\"\"Reuse global `agent` if defined; otherwise build a minimal one.\"\"\"\n",
        "    global agent\n",
        "    try:\n",
        "        agent  # already built in earlier cells\n",
        "        return agent\n",
        "    except NameError:\n",
        "        from langchain_openai import ChatOpenAI\n",
        "        from langchain.agents import create_agent\n",
        "        from langchain_core.tools import tool\n",
        "\n",
        "        @tool\n",
        "        def add(a: float, b: float) -> float:\n",
        "            \"Add two numbers.\"\n",
        "            return a + b\n",
        "\n",
        "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "        agent = create_agent(model=llm, tools=[add], system_prompt=\"You are helpful.\")\n",
        "        return agent\n",
        "\n",
        "# -- chat function used by Gradio --\n",
        "def chat_fn(message, history):\n",
        "    try:\n",
        "        ag = _ensure_agent()\n",
        "        msgs = _to_messages(history, message)\n",
        "        res = ag.invoke({\"messages\": msgs})  # v1 call\n",
        "        return _final_text(res)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# -- UI --\n",
        "try:\n",
        "    demo.close()  # close a previous demo if re-running in the same kernel\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# LangChain Agent Chat\")\n",
        "    gr.Markdown(\"Ask about your KB (kb_search) or general queries. Web search only if TAVILY_API_KEY is set.\")\n",
        "    gr.ChatInterface(chat_fn)\n",
        "    gr.Markdown(\"Tip: Try “Where are tracing docs?” or “Multiply 3.5 and 4.”\")\n",
        "\n",
        "demo.launch(share=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebaf0988",
      "metadata": {
        "id": "ebaf0988"
      },
      "source": [
        "## 6) Custom Tool (@tool) example\n",
        "\n",
        "One simple tool is enough to demonstrate schema and descriptions.  \n",
        "The agent can call this tool if it detects a matching need."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ih42--cA6PR"
      },
      "source": [
        "## 7) Custom tools\n",
        "\n",
        "Define new tools with the `@tool` decorator. Rebuild an agent by passing the updated tools list to `create_agent`, then invoke.\n"
      ],
      "id": "0ih42--cA6PR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1f23058",
      "metadata": {
        "id": "e1f23058"
      },
      "outputs": [],
      "source": [
        "# Custom tool (LangChain v1) — add `multiply`, rebuild agent, return structured JSON\n",
        "\n",
        "import os, json\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import create_agent\n",
        "from langchain_core.tools import tool, create_retriever_tool\n",
        "from langchain_core.messages import AIMessage, ToolMessage\n",
        "\n",
        "# --- Define a custom tool ---\n",
        "@tool\n",
        "def multiply(a: float, b: float) -> float:\n",
        "    \"\"\"Multiply two numbers a and b.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "# --- Compose tools2 (reuse prior tools if they exist) ---\n",
        "tools2: List[Any] = []\n",
        "if \"tools\" in globals():  # from earlier cell\n",
        "    tools2.extend(globals()[\"tools\"])\n",
        "tools2.append(multiply)\n",
        "\n",
        "# Add retriever as a tool if your RAG cell created `retriever` and it's not already added\n",
        "if \"retriever\" in globals() and not any(getattr(t, \"name\", \"\") == \"kb_search\" for t in tools2):\n",
        "    tools2.append(create_retriever_tool(\n",
        "        retriever,\n",
        "        name=\"kb_search\",\n",
        "        description=\"Search the indexed KB/curriculum docs and return relevant passages.\"\n",
        "    ))\n",
        "\n",
        "# Optional Tavily search tool\n",
        "try:\n",
        "    from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "    if os.getenv(\"TAVILY_API_KEY\") and not any(getattr(t, \"name\", \"\") == \"tavily_search_results_json\" for t in tools2):\n",
        "        tools2.append(TavilySearchResults(max_results=5, include_answer=True))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# --- Build an agent (v1) ---\n",
        "try:\n",
        "    llm  # defined earlier?\n",
        "except NameError:\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a helpful technical assistant. Use tools when useful. \"\n",
        "    \"Prefer kb_search for questions about our course/KB; use web search for general web info.\"\n",
        ")\n",
        "agent2 = create_agent(model=llm, tools=tools2, system_prompt=SYSTEM_PROMPT)\n",
        "\n",
        "# --- Helpers to pretty-print structured JSON (answer + tools used + outputs) ---\n",
        "def _final_text(res: Any) -> str:\n",
        "    if isinstance(res, AIMessage):\n",
        "        return res.content or \"\"\n",
        "    if isinstance(res, dict) and \"messages\" in res:\n",
        "        for m in reversed(res[\"messages\"]):\n",
        "            if isinstance(m, AIMessage) or getattr(m, \"type\", \"\") == \"ai\":\n",
        "                return getattr(m, \"content\", \"\") or \"\"\n",
        "    return str(res)\n",
        "\n",
        "def _collect_tool_calls_and_outputs(res: Any, max_len: int = 500) -> List[Dict[str, Any]]:\n",
        "    messages: List[Any] = res.get(\"messages\", []) if isinstance(res, dict) else []\n",
        "\n",
        "    # find tool calls\n",
        "    tool_calls = []\n",
        "    for m in messages:\n",
        "        if isinstance(m, AIMessage) and getattr(m, \"tool_calls\", None):\n",
        "            tool_calls = m.tool_calls or []\n",
        "            break\n",
        "        ak = getattr(m, \"additional_kwargs\", {}) if hasattr(m, \"additional_kwargs\") else {}\n",
        "        if ak.get(\"tool_calls\"):\n",
        "            tool_calls = ak[\"tool_calls\"]; break\n",
        "\n",
        "    # map tool_call_id -> ToolMessage content\n",
        "    outputs = {getattr(m, \"tool_call_id\", None): (getattr(m, \"content\", \"\") or \"\")\n",
        "               for m in messages if isinstance(m, ToolMessage)}\n",
        "\n",
        "    def trunc(s: Optional[str]) -> str:\n",
        "        if not s: return \"\"\n",
        "        return s[:max_len] + (\"…\" if len(s) > max_len else \"\")\n",
        "\n",
        "    return [{\"name\": c.get(\"name\"), \"args\": c.get(\"args\"), \"output\": trunc(outputs.get(c.get(\"id\")))}\n",
        "            for c in tool_calls]\n",
        "\n",
        "def _usage(res: Any) -> Optional[Dict[str, Any]]:\n",
        "    if isinstance(res, AIMessage):\n",
        "        return getattr(res, \"response_metadata\", {}).get(\"token_usage\")\n",
        "    if isinstance(res, dict) and \"messages\" in res:\n",
        "        for m in reversed(res[\"messages\"]):\n",
        "            meta = getattr(m, \"response_metadata\", {}) if hasattr(m, \"response_metadata\") else {}\n",
        "            if meta.get(\"token_usage\"):\n",
        "                return meta[\"token_usage\"]\n",
        "    return None\n",
        "\n",
        "def to_structured(res: Any) -> Dict[str, Any]:\n",
        "    return {\"answer\": _final_text(res), \"tools\": _collect_tool_calls_and_outputs(res), \"usage\": _usage(res)}\n",
        "\n",
        "# --- Demo ---\n",
        "q = \"Multiply 3.5 by 4 and then list two LangSmith evaluation features.\"\n",
        "res = agent2.invoke({\"messages\": [{\"role\": \"user\", \"content\": q}]})\n",
        "print(json.dumps(to_structured(res), indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfc3eae7",
      "metadata": {
        "id": "dfc3eae7"
      },
      "source": [
        "## 7) Wrap-up & Next steps\n",
        "\n",
        "You built an end-to-end app:\n",
        "- Baseline LLM → **Prompted chain** → **RAG** → **Agent with tools** → **(Optional) UI**\n",
        "- Re-used the **same retriever** everywhere (built once).\n",
        "- Optionally enabled **LangSmith** tracing for observability.\n",
        "\n",
        "**Ideas to extend:**\n",
        "- Swap FAISS for your vector DB of choice.  \n",
        "- Add **validators** (output schemas) and **evaluation** suites.  \n",
        "- Add domain-specific tools (databases, calculators, internal APIs).\n",
        "\n",
        "> If you want to run the chat UI: uncomment `demo.launch()` in Section 5."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}